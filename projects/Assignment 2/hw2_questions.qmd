---
title: "Poisson Regression Examples"
author: "Allen Abraham"
date: today
callout-appearance: minimal # this hides the blue "i" icon on .callout-notes
editor_options: 
  chunk_output_type: console
format: html

---


## Blueprinty Case Study

### Introduction

Blueprinty is a small firm that makes software for developing blueprints specifically for submitting patent applications to the US patent office. Their marketing team would like to make the claim that patent applicants using Blueprinty's software are more successful in getting their patent applications approved. Ideal data to study such an effect might include the success rate of patent applications before using Blueprinty's software and after using it. unfortunately, such data is not available. 

However, Blueprinty has collected data on 1,500 mature (non-startup) engineering firms. The data include each firm's number of patents awarded over the last 5 years, regional location, age since incorporation, and whether or not the firm uses Blueprinty's software. The marketing team would like to use this data to make the claim that firms using Blueprinty's software are more successful in getting their patent applications approved.


### Data

We will start by reading in the data and conducting some exploratory data analysis to understand the data better.

```{python}
import pandas as pd

data = pd.read_csv("data/blueprinty.csv")
data.head()
```


```{python}
import matplotlib.pyplot as plt

mean_patents_customer = data[data['iscustomer'] == 1]['patents'].mean()
mean_patents_not_customer = data[data['iscustomer'] == 0]['patents'].mean()

data.groupby("iscustomer")["patents"].hist(alpha=0.5)
plt.xlabel("Number of Patents")
plt.ylabel("Frequency")
#plot mean as vertical line 
plt.axvline(mean_patents_customer, color='orange', linestyle='dashed', linewidth=1)
plt.axvline(mean_patents_not_customer, color='blue', linestyle='dashed', linewidth=1)
#add legned for mean
plt.legend(["Average patents (customer)", "Average patents (non-customer)", " Not Customer", "Customer"])
plt.show()

print("Average number of patents for Blueprinty customers: ", mean_patents_customer)
print("Average number of patents for non-customers: ", mean_patents_not_customer)

```

We will conduct a t-test to determine if the difference in the number of patents awarded is statistically significant by customer status (whether the firm is a customer of Blueprinty or not).

```{python}
from scipy.stats import ttest_ind

ttest_ind(data[data['iscustomer'] == 1]['patents'], data[data['iscustomer'] == 0]['patents'])
```

The histogram show that there are a lot more non-customers than customers in the dataset. The average number of patents for Blueprinty customers is slightly higher at 4.09 than for non-customers at 3.62. The t-test shows that this difference is statistically significant at the 5% level. However, this analysis does not account for other factors that may influence the number of patents awarded, such as the age and regional location of the firms.

Blueprinty customers are not selected at random. It may be important to account for systematic differences in the age and regional location of customers vs non-customers.

We will compare the differences in regions and ages by customer status. We will also conduct t-tests to determine if the differences in proportions of firms in each region and the difference in means of firm ages are statistically significant by customer status.

```{python}
data.groupby("iscustomer")["region"].value_counts(normalize=True).unstack().plot(kind='bar', stacked=True)
plt.ylabel("Proportion of Firms")
plt.xlabel("Customer Status")
plt.title("Proportion of Firms by Region")
plt.xticks([0, 1], ['Not Customer', 'Customer'], rotation=0)
plt.show()

mean_age_customer = data[data['iscustomer'] == 1]['age'].mean()
mean_age_not_customer = data[data['iscustomer'] == 0]['age'].mean()

data.groupby("iscustomer")["age"].hist(alpha=0.5)
plt.xlabel("Age")
plt.ylabel("Frequency")
#plot mean as vertical line
plt.axvline(mean_age_customer, color='orange', linestyle='dashed', linewidth=1)
plt.axvline(mean_age_not_customer, color='blue', linestyle='dashed', linewidth=1)
plt.legend(["Mean Age (customer)", "Mean Age (non-customer)", "Not Customer", "Customer"])
plt.show()

print("Average firm age of Blueprinty customers: ", mean_age_customer)
print("Average firm age of non-customers: ", mean_age_not_customer)

```

We conduct multiple t-tests to determine if the difference in proportions of firms in each region is statistically significant by customer status.

```{python}

# Calculate the proportion of firms in each region by customer status
region_proportions = data.groupby("iscustomer")["region"].value_counts(normalize=True).unstack(fill_value=0)
print("Region Proportions by Customer Status:")
print(region_proportions)

# T-test for difference in proportions of firms in each region by customer status for each region
results = {}
for region in region_proportions.columns:
    # Extracting boolean arrays where each region equals the current region for customers and non-customers
    customer_region = data[data['iscustomer'] == 1]['region'] == region
    non_customer_region = data[data['iscustomer'] == 0]['region'] == region
    
    # Performing the t-test
    t_stat, p_value = ttest_ind(customer_region, non_customer_region)
    results[region] = (t_stat, p_value)

results
```

We conduct a t-test to determine if the difference in means of firm ages is statistically significant by customer status.

```{python}
ttest_ind(data[data['iscustomer'] == 1]['age'], data[data['iscustomer'] == 0]['age'])

```

The t-tests shows that the difference in proportions of firms in each region is statistically significant for the Midwest, Northeast, and Northwest regions at the 5% level. The t-test on average age of the firm shows that the difference in means of firm ages is statistically significant at the 5% level. 

Therefore, we should account for these differences in our analysis. We cannot conclude that the difference in the number of patents awarded is due to Blueprinty's software without controlling for these differences.


### Estimation of Simple Poisson Model

Since our outcome variable of interest can only be small integer values per a set unit of time, we can use a Poisson density to model the number of patents awarded to each engineering firm over the last 5 years. We start by estimating a simple Poisson model via Maximum Likelihood.

We will write down the log-likelihood function for $Y \sim \text{Poisson}(\lambda)$ given the Poisson probability density function of $(Y|\lambda) = e^{-\lambda}\lambda^Y/Y!$.

The log-likelihood function for a sample $( Y_1, Y_2, \ldots, Y_n)$ from a Poisson distribution with parameter $( \lambda)$ is given by:

$\ell(\lambda; y_1, y_2, \ldots, y_n) = \sum_{i=1}^n y_i \log(\lambda) - n\lambda - \sum_{i=1}^n \log(y_i!)$



Next, we will code the log-likelihood function for the Poisson model. This is a function of lambda and Y. The function should return the log-likelihood value for a given rate parameter lambda and a set of observed counts Y. 

```{python}
import numpy as np

def poisson_loglikelihood(lambda_, Y):
    """
    Calculate the log likelihood for a Poisson distributed set of observed counts, Y, given a rate lambda_,
    without using gammaln for factorial computation.

    Parameters:
    lambda_ : float
        The rate parameter of the Poisson distribution (lambda > 0).
    Y : array_like
        Array of observed counts (non-negative integers).

    Returns:
    float
        The log likelihood value.
    """
    if lambda_ <= 0:
        return -np.inf  # log likelihood is undefined for non-positive lambda
    Y = np.asarray(Y)
    log_factorials = np.array([np.sum(np.log(np.arange(1, y+1))) if y > 0 else 0 for y in Y])
    return np.sum(Y * np.log(lambda_) - lambda_ - log_factorials)

```



We will use the above function to plot a range of  lambdas (0.1-10) on the horizontal axis and the log likelihood on the vertical axis. We will use the observed patent counts as Y. The maximum likelihood estimate (MLE) of lambda is the value that maximizes the log-likelihood function.

```{python}
Y = data['patents'].values  # Extracting patent counts

# Define the range of lambda values from a small positive number up to a reasonable upper limit
lambda_values = np.linspace(0.1, 10, 400)  # from 0.1 to 10, 400 points
log_likelihoods = [poisson_loglikelihood(l, Y) for l in lambda_values]

# Plotting the results
plt.plot(lambda_values, log_likelihoods, label='Log-Likelihood')
plt.title('Log-Likelihood of Lambda for Observed Patent Counts')
plt.xlabel('Lambda (Rate Parameter)')
plt.ylabel('Log-Likelihood')
plt.grid(True)
plt.legend()
plt.show()
```


We will use optimization to find the MLE of lambda. We will use the `scipy.optimize.minimize` function to minimize the negative log-likelihood function. The negative log-likelihood is the negative of the log-likelihood function, which we want to minimize. We will set the bounds of the optimization to ensure that lambda is positive. 

We will also calculate the mean number of patents per firm which should be close to the MLE of lambda since the Poisson distribution has only one parameter which is the rate parameter lambda. The mean of a Poisson distribution is equal to its rate parameter.

```{python}
from scipy.optimize import minimize
from scipy.special import gammaln

def negative_log_likelihood(lambda_, Y):
    if lambda_ <= 0:  # Log-likelihood undefined for non-positive lambda values
        return np.inf
    return -np.sum(Y * np.log(lambda_) - lambda_ - gammaln(Y + 1))

Y =  data['patents'].values

# Find MLE for lambda using optimization
result = minimize(fun=negative_log_likelihood, x0=np.array([1]), args=(Y,), bounds=[(0.1, None)])

# Output the results
if result.success:
    print(f"MLE for lambda (lambda_mle): {result.x[0]}")
else:
    print("Optimization failed:", result.message)


mean_patents = np.mean(Y)
print("Mean number of patents per firm: ", mean_patents)

```
As expected, the MLE of lambda is the same as the average number of patents per firm. This is because the Poisson distribution has only one parameter, the rate parameter lambda, which is equal to the mean of the distribution. 


### Estimation of Poisson Regression Model

Next, we extend our simple Poisson model to a Poisson Regression Model such that $Y_i = \text{Poisson}(\lambda_i)$ where $\lambda_i = \exp(X_i'\beta)$. The interpretation is that the success rate of patent awards is not constant across all firms ($\lambda$) but rather is a function of firm characteristics $X_i$. Specifically, we will use the covariates age, age squared, region, and whether the firm is a customer of Blueprinty.

We will Update our log-likelihood function with an additional argument to take in a covariate matrix X. We will also change the parameter of the model from lambda to the beta vector. In this model, lambda must be a positive number, so we choose the inverse link function g() to be exp() so that_ $\lambda_i = e^{X_i'\beta}$. 

We will make sure that the first column of X are all 1's to enable a constant term in the model, and the subsequent columns are age, age squared, binary variables for all but one of the regions, and the binary iscustomer variable.


```{python}
#| message: false
#| echo: true
import statsmodels.api as sm

# Prepare the covariate matrix X
data['age_squared'] = data['age'] ** 2  # Add age squared
data = pd.get_dummies(data, columns=['region'], drop_first=True)  # One-hot encode region

X = np.column_stack([
    np.ones(len(data)),  # Intercept
    data['age'],
    data['age_squared'],
    data['iscustomer']
] + [data[col] for col in data.columns if 'region_' in col])

Y = data['patents'].values

def neg_log_likelihood(beta, Y, X):
    eta = np.dot(X, beta)
    lambda_i = np.exp(eta)
    log_likelihood = np.sum(Y * np.log(lambda_i) - lambda_i - gammaln(Y + 1))
    return -log_likelihood  # Minimize this



beta_initial = np.zeros(X.shape[1])  # Initial guess for the parameters

neg_log_likelihood(beta_initial, Y, X)


``` 


Next, we will use the updated function with scipy.optimize to find the MLE vector and the Hessian of the Poisson model with covariates. We will use the Hessian to find standard errors of the beta parameter.

```{python}
from scipy.optimize import minimize
import numpy as np

# Scale features
from sklearn.preprocessing import StandardScaler
scaler = StandardScaler()
X_scaled = scaler.fit_transform(X[:, 1:])  # Assuming first column is intercept
X_scaled = np.hstack((np.ones((X.shape[0], 1)), X_scaled))  # Add intercept back

# Change optimization method and adjust initial guesses
beta_initial = np.random.normal(loc=0.0, scale=0.1, size=X.shape[1])  #


result = minimize(
    neg_log_likelihood, beta_initial, args=(Y, X_scaled),
    method='L-BFGS-B', options={'disp': True, 'maxiter': 500}
)

if result.success:
    beta_mle = result.x
    print("MLE of beta:", beta_mle)
else:
    print("Optimization failed:", result.message)

```

```{python}
hessian_matrix = result.hess_inv.todense()
std_errors = np.sqrt(np.diag(hessian_matrix))
print("Standard errors:", std_errors)

```

```{python}
coefficients_table = pd.DataFrame({
    'Coefficient': beta_mle,
    'Standard Error': std_errors
}, index=['Intercept', 'Age', 'Age Squared', 'Is Customer'] + [f'Region_{i}' for i in range(len(std_errors) - 4)])
print(coefficients_table)

```


We will verify the results using Python's sm.GLM() function.

```{python}
import statsmodels.api as sm

# Create the GLM model with Poisson family
poisson_model = sm.GLM(Y, X_scaled, family=sm.families.Poisson())

# Fit the model
result = poisson_model.fit()

# Print the summary of the model to see the results
print(result.summary())

```

Based on the Poisson regression model, the coefficients provide insights into the relationship between the covariates and the number of patents awarded. The coefficients for age and age squared are large and very significant suggesting that age of the firm has a significant impact on the number of patents awarded. The coefficient for the iscustomer variable is positive, indicating that firms using Blueprinty's software tend to have more patents awarded but the coefficient is much smaller than the age coefficient, suggesting that age is a more significant factor. The coefficients for the region variables are small and not statistically significant, indicating that regional location may not have a significant impact on the number of patents awarded.

 Even though the average firm age of Blueprinty customers is lower than non-customers, the average number of patents awarded to customers is higher. This suggests that Blueprinty's software may be associated with higher patent awards even after accounting for the age factor. 



## AirBnB Case Study

### Introduction

AirBnB is a popular platform for booking short-term rentals. In March 2017, students Annika Awad, Evan Lebo, and Anna Linden scraped of 40,000 Airbnb listings from New York City.  The data include the following variables:

:::: {.callout-note collapse="true"}
### Variable Definitions

    - `id` = unique ID number for each unit
    - `last_scraped` = date when information scraped
    - `host_since` = date when host first listed the unit on Airbnb
    - `days` = `last_scraped` - `host_since` = number of days the unit has been listed
    - `room_type` = Entire home/apt., Private room, or Shared room
    - `bathrooms` = number of bathrooms
    - `bedrooms` = number of bedrooms
    - `price` = price per night (dollars)
    - `number_of_reviews` = number of reviews for the unit on Airbnb
    - `review_scores_cleanliness` = a cleanliness score from reviews (1-10)
    - `review_scores_location` = a "quality of location" score from reviews (1-10)
    - `review_scores_value` = a "quality of value" score from reviews (1-10)
    - `instant_bookable` = "t" if instantly bookable, "f" if not

::::


_todo: Assume the number of reviews is a good proxy for the number of bookings. Perform some exploratory data analysis to get a feel for the data, handle or drop observations with missing values on relevant variables, build one or more models (e.g., a poisson regression model for the number of bookings as proxied by the number of reviews), and interpret model coefficients to describe variation in the number of reviews as a function of the variables provided._

We will start by reading in the data

```{python}
airbnb_data = pd.read_csv("data/airbnb.csv")

# Display the first few rows and summary statistics
airbnb_data.head(), airbnb_data.describe()
```

We will perform some data cleaning and transformation. We will convert date columns to datetime format, remove rows with missing values in host_since column since it has only 35 missing values, fill missing values for bathrooms and bedrooms (160 and 76 missing rows, respectively) with median, handle missing review scores by filling with median, convert instant_bookable to boolean, and remove the unnamed: 0 column.

```{python}
# Convert date columns to datetime format
airbnb_data['last_scraped'] = pd.to_datetime(airbnb_data['last_scraped'], format='%m/%d/%Y')
airbnb_data['host_since'] = pd.to_datetime(airbnb_data['host_since'], format='%m/%d/%Y')

# remove rows with missing values in host_since column
airbnb_data.dropna(subset=['host_since'], inplace=True)

# Fill missing values for bathrooms and bedrooms with median
airbnb_data['bathrooms'].fillna(airbnb_data['bathrooms'].median(), inplace=True)
airbnb_data['bedrooms'].fillna(airbnb_data['bedrooms'].median(), inplace=True)

# Handling missing review scores by filling with median
for column in ['review_scores_cleanliness', 'review_scores_location', 'review_scores_value']:
    airbnb_data[column].fillna(airbnb_data[column].median(), inplace=True)

# convert instant_bookable to boolean. 't' = True, 'f' = False
airbnb_data['instant_bookable'] = airbnb_data['instant_bookable'].map({'t': True, 'f': False})

# remove unnamed: 0 column
airbnb_data.drop('Unnamed: 0', axis=1, inplace=True)

# Check for any remaining missing values and verify transformations
airbnb_data.isnull().sum(), airbnb_data.head()

```

We will perform some exploratory data analysis to get a feel for the data. We will plot histograms of key variables and a boxplot of price by room type.

```{python}
import matplotlib.pyplot as plt
import seaborn as sns

# Setting up the aesthetics for plots
sns.set(style="whitegrid")

# Creating a figure to plot distributions of key variables
fig, axes = plt.subplots(2, 2, figsize=(14, 12))

# Distribution of 'number_of_reviews'
sns.histplot(airbnb_data['number_of_reviews'], bins=30, ax=axes[0, 0], kde=False, color='blue')
axes[0, 0].set_title('Distribution of Number of Reviews')
axes[0, 0].set_xlabel('Number of Reviews')
axes[0, 0].set_ylabel('Frequency')

# Distribution of 'price'
sns.histplot(airbnb_data['price'], bins=30, ax=axes[0, 1], kde=False, color='green')
axes[0, 1].set_title('Distribution of Price')
axes[0, 1].set_xlabel('Price ($ per night)')
axes[0, 1].set_ylabel('Frequency')

# Distribution of 'review_scores_cleanliness'
sns.histplot(airbnb_data['review_scores_cleanliness'], bins=9, ax=axes[1, 0], kde=False, color='red')
axes[1, 0].set_title('Distribution of Cleanliness Scores')
axes[1, 0].set_xlabel('Cleanliness Score')
axes[1, 0].set_ylabel('Frequency')

# Boxplot of price by room type
sns.boxplot(data=airbnb_data, x='room_type', y='price', ax=axes[1, 1])
axes[1, 1].set_title('Price Distribution by Room Type')
axes[1, 1].set_xlabel('Room Type')
axes[1, 1].set_ylabel('Price ($ per night)')

plt.tight_layout()
plt.show()

```



Finally, we will build a Poisson regression model for the number of reviews as a function of the variables provided. We will use the `number_of_reviews` as the dependent variable and `days`, `room_type`, `bathrooms`, `bedrooms`, `price`, `review_scores_cleanliness`, `review_scores_location`, `review_scores_value`, and `instant_bookable` as independent variables. We will one-hot encode the `room_type` variable, standardize the features, and use the `scipy.optimize.minimize` function to find the MLE of the beta coefficients.


```{python}

from math import exp

# Drop rows with missing values in relevant columns
columns_to_keep = ['days', 'room_type', 'bathrooms', 'bedrooms', 'price', 
                   'review_scores_cleanliness', 'review_scores_location', 
                   'review_scores_value', 'instant_bookable', 'number_of_reviews']

data_clean = airbnb_data[columns_to_keep]

# One-hot encode categorical variables
data_clean = pd.get_dummies(data_clean, columns=['room_type'], drop_first=True)


# Separate features and target
features = [col for col in data_clean.columns if col != 'number_of_reviews']
X = data_clean[features].values
Y = data_clean['number_of_reviews'].values

# Add intercept column
X = np.column_stack([np.ones(X.shape[0]), X])

# Standardize features (excluding intercept)
scaler = StandardScaler()
X[:, 1:] = scaler.fit_transform(X[:, 1:])

# Define a custom function for exponentiation
def custom_exp(arr):
    return np.array([exp(x) for x in arr])

# Define the negative log-likelihood function for Poisson regression
def neg_log_likelihood(beta, Y, X):
    eta = np.dot(X, beta)
    lambda_i = custom_exp(eta)  # Use the custom exponentiation function
    log_likelihood = np.sum(Y * np.log(lambda_i) - lambda_i - gammaln(Y + 1))
    return -log_likelihood

# Initial guess for parameters
beta_initial = np.zeros(X.shape[1])

# Optimize to find the MLE of beta
result = minimize(neg_log_likelihood, beta_initial, args=(Y, X), method='L-BFGS-B', options={'disp': True, 'maxiter': 500})

# Extract results
if result.success:
    beta_mle = result.x
else:
    print("Optimization failed:", result.message)

# Get coefficient names
coeff_names = ['Intercept'] + features

# Create a DataFrame to hold coefficients
coef_df = pd.DataFrame({'Coefficient': coeff_names, 'Estimate': beta_mle})

# Display the coefficients
print(coef_df)
```

### Interpretation of Results

The Poisson regression model provides estimates for the coefficients of the model. The coefficients represent the log of the rate of reviews for each unit. The interpretation of the coefficients is as follows:

**Intercept (2.64):** Represents the baseline log count of bookings (as proxied by reviews) for the reference level of categorical variables when all other variables are at their reference levels or zero.

**Days (0.40):** A positive coefficient indicates that as the number of days since the listing was created increases, the expected number of bookings increases. Specifically, for each additional day, the expected log count of bookings increases by approximately 0.40. This suggests that older listings tend to have more bookings, possibly due to accumulating reviews and gaining more visibility.

**Bathrooms (-0.04):** A negative coefficient means that as the number of bathrooms increases, the expected number of bookings decreases. This might suggest that the number of bathrooms isn't a significant factor influencing bookings or that properties with more bathrooms have a different appeal.

**Bedrooms (0.05):** A positive coefficient suggests that as the number of bedrooms increases, the expected number of bookings increases. This implies that larger properties (in terms of bedrooms) tend to attract more bookings, likely because they accommodate more guests.

**Price (-0.05):** A small negative coefficient suggests that an increase in the listing price is associated with a slight decrease in the expected number of bookings. Higher prices may deter some potential customers, leading to fewer bookings.


**Review Score Cleanliness (0.04):** A positive coefficient indicates that as the cleanliness score increases, the expected number of bookings increases. This means that customers value cleanliness, and highly-rated properties tend to have more bookings.

**Review Score Location (-0.14):** A negative coefficient implies that higher location scores are associated with fewer bookings. While counterintuitive, this may indicate that other factors might be overshadowing location, or that listings in prime locations are more competitive.

**Review Score Value (-0.13):** A negative coefficient means that higher value scores are associated with fewer bookings. This could imply that while customers value "value for money," this factor alone may not lead to more bookings.

**Instant Bookable (0.19):** Listings that are instantly bookable tend to have more bookings than those that are not. This suggests that ease of booking is an important factor for customers when choosing properties.

**Room Type: Private Room (-0.05):** Listings with private rooms have fewer bookings compared to the reference category (entire home/apt). This suggests private rooms may be less attractive to a larger customer base.
    
**Room Type: Shared Room (-0.04):**  Listings with shared rooms also have fewer bookings than the reference category. This indicates that shared rooms appeal to a niche market, leading to fewer bookings.


### Conclusion

The most important factors influencing the number of bookings (as proxied by reviews) for Airbnb listings in New York City include the number of days since the listing was created, and whether the listing is instantly bookable. These factors have the largest impact on bookings, with older listings and instantly bookable listings attracting more bookings. Other factors such as the number of bedrooms and bathrooms, price, and room type had a relatively smaller impact on bookings.There were some unexpected results, such as the negative coefficients for location and value review scores, which may require further investigation. 

The model provides insights into the factors that drive bookings and can help hosts optimize their listings for better performance.


